{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301ab0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CIC-IDS2017 Data Preprocessing\n",
      "============================================================\n",
      "âœ… Loaded 3,119,345 records with 86 features\n",
      "âœ… Cleaned 86 column names\n",
      "Found missing values in 85 columns:\n",
      "  Flow_ID: 288,602 (9.25%)\n",
      "  Source_IP: 288,602 (9.25%)\n",
      "  Source_Port: 288,602 (9.25%)\n",
      "  Destination_IP: 288,602 (9.25%)\n",
      "  Destination_Port: 288,602 (9.25%)\n",
      "  Protocol: 288,602 (9.25%)\n",
      "  Timestamp: 288,602 (9.25%)\n",
      "  Flow_Duration: 288,602 (9.25%)\n",
      "  Total_Fwd_Packets: 288,602 (9.25%)\n",
      "  Total_Backward_Packets: 288,602 (9.25%)\n",
      "  Total_Length_of_Fwd_Packets: 288,602 (9.25%)\n",
      "  Total_Length_of_Bwd_Packets: 288,602 (9.25%)\n",
      "  Fwd_Packet_Length_Max: 288,602 (9.25%)\n",
      "  Fwd_Packet_Length_Min: 288,602 (9.25%)\n",
      "  Fwd_Packet_Length_Mean: 288,602 (9.25%)\n",
      "  Fwd_Packet_Length_Std: 288,602 (9.25%)\n",
      "  Bwd_Packet_Length_Max: 288,602 (9.25%)\n",
      "  Bwd_Packet_Length_Min: 288,602 (9.25%)\n",
      "  Bwd_Packet_Length_Mean: 288,602 (9.25%)\n",
      "  Bwd_Packet_Length_Std: 288,602 (9.25%)\n",
      "  Flow_Bytes_per_s: 289,960 (9.30%)\n",
      "  Flow_Packets_per_s: 288,602 (9.25%)\n",
      "  Flow_IAT_Mean: 288,602 (9.25%)\n",
      "  Flow_IAT_Std: 288,602 (9.25%)\n",
      "  Flow_IAT_Max: 288,602 (9.25%)\n",
      "  Flow_IAT_Min: 288,602 (9.25%)\n",
      "  Fwd_IAT_Total: 288,602 (9.25%)\n",
      "  Fwd_IAT_Mean: 288,602 (9.25%)\n",
      "  Fwd_IAT_Std: 288,602 (9.25%)\n",
      "  Fwd_IAT_Max: 288,602 (9.25%)\n",
      "  Fwd_IAT_Min: 288,602 (9.25%)\n",
      "  Bwd_IAT_Total: 288,602 (9.25%)\n",
      "  Bwd_IAT_Mean: 288,602 (9.25%)\n",
      "  Bwd_IAT_Std: 288,602 (9.25%)\n",
      "  Bwd_IAT_Max: 288,602 (9.25%)\n",
      "  Bwd_IAT_Min: 288,602 (9.25%)\n",
      "  Fwd_PSH_Flags: 288,602 (9.25%)\n",
      "  Bwd_PSH_Flags: 288,602 (9.25%)\n",
      "  Fwd_URG_Flags: 288,602 (9.25%)\n",
      "  Bwd_URG_Flags: 288,602 (9.25%)\n",
      "  Fwd_Header_Length: 288,602 (9.25%)\n",
      "  Bwd_Header_Length: 288,602 (9.25%)\n",
      "  Fwd_Packets_per_s: 288,602 (9.25%)\n",
      "  Bwd_Packets_per_s: 288,602 (9.25%)\n",
      "  Min_Packet_Length: 288,602 (9.25%)\n",
      "  Max_Packet_Length: 288,602 (9.25%)\n",
      "  Packet_Length_Mean: 288,602 (9.25%)\n",
      "  Packet_Length_Std: 288,602 (9.25%)\n",
      "  Packet_Length_Variance: 288,602 (9.25%)\n",
      "  FIN_Flag_Count: 288,602 (9.25%)\n",
      "  SYN_Flag_Count: 288,602 (9.25%)\n",
      "  RST_Flag_Count: 288,602 (9.25%)\n",
      "  PSH_Flag_Count: 288,602 (9.25%)\n",
      "  ACK_Flag_Count: 288,602 (9.25%)\n",
      "  URG_Flag_Count: 288,602 (9.25%)\n",
      "  CWE_Flag_Count: 288,602 (9.25%)\n",
      "  ECE_Flag_Count: 288,602 (9.25%)\n",
      "  Down_per_Up_Ratio: 288,602 (9.25%)\n",
      "  Average_Packet_Size: 288,602 (9.25%)\n",
      "  Avg_Fwd_Segment_Size: 288,602 (9.25%)\n",
      "  Avg_Bwd_Segment_Size: 288,602 (9.25%)\n",
      "  Fwd_Header_Length.1: 288,602 (9.25%)\n",
      "  Fwd_Avg_Bytes_per_Bulk: 288,602 (9.25%)\n",
      "  Fwd_Avg_Packets_per_Bulk: 288,602 (9.25%)\n",
      "  Fwd_Avg_Bulk_Rate: 288,602 (9.25%)\n",
      "  Bwd_Avg_Bytes_per_Bulk: 288,602 (9.25%)\n",
      "  Bwd_Avg_Packets_per_Bulk: 288,602 (9.25%)\n",
      "  Bwd_Avg_Bulk_Rate: 288,602 (9.25%)\n",
      "  Subflow_Fwd_Packets: 288,602 (9.25%)\n",
      "  Subflow_Fwd_Bytes: 288,602 (9.25%)\n",
      "  Subflow_Bwd_Packets: 288,602 (9.25%)\n",
      "  Subflow_Bwd_Bytes: 288,602 (9.25%)\n",
      "  Init_Win_bytes_forward: 288,602 (9.25%)\n",
      "  Init_Win_bytes_backward: 288,602 (9.25%)\n",
      "  act_data_pkt_fwd: 288,602 (9.25%)\n",
      "  min_seg_size_forward: 288,602 (9.25%)\n",
      "  Active_Mean: 288,602 (9.25%)\n",
      "  Active_Std: 288,602 (9.25%)\n",
      "  Active_Max: 288,602 (9.25%)\n",
      "  Active_Min: 288,602 (9.25%)\n",
      "  Idle_Mean: 288,602 (9.25%)\n",
      "  Idle_Std: 288,602 (9.25%)\n",
      "  Idle_Max: 288,602 (9.25%)\n",
      "  Idle_Min: 288,602 (9.25%)\n",
      "  Label: 288,602 (9.25%)\n",
      "âœ… Missing values handled\n",
      "Handled infinite values in 2 columns:\n",
      "  Flow_Bytes_per_s: 1,509 infinite values\n",
      "  Flow_Packets_per_s: 2,867 infinite values\n",
      "âœ… Removed 288,804 duplicate records\n",
      "âœ… Sampled to 100,000 records\n",
      "âœ… Binary target created:\n",
      "  Normal: 80,210 (80.2%)\n",
      "  Attack: 19,790 (19.8%)\n",
      "Found 5 categorical columns to encode:\n",
      "  Flow_ID: 85486 unique values\n",
      "  Source_IP: 4618 unique values\n",
      "  Destination_IP: 7179 unique values\n",
      "  Timestamp: 11459 unique values\n",
      "  source_file: 8 unique values\n",
      "âœ… Categorical encoding complete\n",
      "âœ… Created 3 new features:\n",
      "  Packet_Ratio\n",
      "  Log_Flow_Bytes_per_s\n",
      "  Log_Fwd_Avg_Bytes_per_Bulk\n",
      "âœ… Scaled 88 numerical features\n",
      "\n",
      "==================================================\n",
      "Final dataset shape: (100000, 91)\n",
      "Features for modeling: 88\n",
      "ðŸ’¾ Processed data saved to: processed_network_data.csv\n",
      "\n",
      "ðŸ“Š PREPROCESSING SUMMARY:\n",
      "Records: 100,000\n",
      "Features: 88\n",
      "Attack Rate: 19.79%\n"
     ]
    }
   ],
   "source": [
    "#Data Cleaning and Preprocessing\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class NetworkDataPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessor for CIC-IDS2017 network traffic data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = RobustScaler()  # Better for outliers than StandardScaler\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.feature_columns = None\n",
    "        self.target_column = 'Label'\n",
    "        self.processed_data = None\n",
    "        \n",
    "    def load_data(self, file_path):\n",
    "        \"\"\"Load the dataset\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(file_path)\n",
    "            print(f\"âœ… Loaded {self.df.shape[0]:,} records with {self.df.shape[1]} features\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading data: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def clean_column_names(self):\n",
    "        \"\"\"Clean and standardize column names\"\"\"\n",
    "        \n",
    "        # Remove leading/trailing spaces and replace spaces with underscores\n",
    "        self.df.columns = self.df.columns.str.strip().str.replace(' ', '_')\n",
    "        \n",
    "        # Handle special characters\n",
    "        self.df.columns = self.df.columns.str.replace('/', '_per_')\n",
    "        self.df.columns = self.df.columns.str.replace('-', '_')\n",
    "        \n",
    "        print(f\"âœ… Cleaned {len(self.df.columns)} column names\")\n",
    "        return self.df.columns.tolist()\n",
    "    \n",
    "    def handle_missing_values(self):\n",
    "        \"\"\"Handle missing values in the dataset\"\"\"\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_summary = self.df.isnull().sum()\n",
    "        missing_cols = missing_summary[missing_summary > 0]\n",
    "        \n",
    "        if len(missing_cols) > 0:\n",
    "            print(f\"Found missing values in {len(missing_cols)} columns:\")\n",
    "            for col, count in missing_cols.items():\n",
    "                pct = (count / len(self.df)) * 100\n",
    "                print(f\"  {col}: {count:,} ({pct:.2f}%)\")\n",
    "            \n",
    "            # Strategy for handling missing values\n",
    "            numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "            categorical_cols = self.df.select_dtypes(include=['object']).columns\n",
    "            \n",
    "            # Fill numeric columns with median\n",
    "            for col in numeric_cols:\n",
    "                if col in missing_cols:\n",
    "                    median_val = self.df[col].median()\n",
    "                    self.df[col].fillna(median_val, inplace=True)\n",
    "            \n",
    "            # Fill categorical columns with mode\n",
    "            for col in categorical_cols:\n",
    "                if col in missing_cols and col != self.target_column:\n",
    "                    mode_val = self.df[col].mode()[0] if not self.df[col].mode().empty else 'Unknown'\n",
    "                    self.df[col].fillna(mode_val, inplace=True)\n",
    "        \n",
    "        print(\"âœ… Missing values handled\")\n",
    "        return missing_summary\n",
    "    \n",
    "    def handle_infinite_values(self):\n",
    "        \"\"\"Handle infinite values in numeric columns\"\"\"\n",
    "        \n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        inf_counts = {}\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            inf_count = np.isinf(self.df[col]).sum()\n",
    "            if inf_count > 0:\n",
    "                inf_counts[col] = inf_count\n",
    "                \n",
    "                # Replace infinite values with NaN, then fill with median\n",
    "                self.df[col] = self.df[col].replace([np.inf, -np.inf], np.nan)\n",
    "                median_val = self.df[col].median()\n",
    "                self.df[col].fillna(median_val, inplace=True)\n",
    "        \n",
    "        if inf_counts:\n",
    "            print(f\"Handled infinite values in {len(inf_counts)} columns:\")\n",
    "            for col, count in inf_counts.items():\n",
    "                print(f\"  {col}: {count:,} infinite values\")\n",
    "        else:\n",
    "            print(\"âœ… No infinite values found\")\n",
    "        \n",
    "        return inf_counts\n",
    "    \n",
    "    def remove_duplicates(self):\n",
    "        \"\"\"Remove duplicate records\"\"\"\n",
    "        \n",
    "        initial_count = len(self.df)\n",
    "        self.df.drop_duplicates(inplace=True)\n",
    "        final_count = len(self.df)\n",
    "        \n",
    "        removed = initial_count - final_count\n",
    "        if removed > 0:\n",
    "            print(f\"âœ… Removed {removed:,} duplicate records\")\n",
    "        else:\n",
    "            print(\"âœ… No duplicates found\")\n",
    "        \n",
    "        return removed\n",
    "    \n",
    "    def encode_categorical_features(self):\n",
    "        \"\"\"Encode categorical features\"\"\"\n",
    "        \n",
    "        \n",
    "        categorical_cols = self.df.select_dtypes(include=['object']).columns\n",
    "        categorical_cols = [col for col in categorical_cols if col != self.target_column]\n",
    "        \n",
    "        if len(categorical_cols) > 0:\n",
    "            print(f\"Found {len(categorical_cols)} categorical columns to encode:\")\n",
    "            for col in categorical_cols:\n",
    "                unique_vals = self.df[col].nunique()\n",
    "                print(f\"  {col}: {unique_vals} unique values\")\n",
    "                \n",
    "                # Use Label Encoding for now (you might want One-Hot for low cardinality)\n",
    "                le = LabelEncoder()\n",
    "                self.df[col] = le.fit_transform(self.df[col].astype(str))\n",
    "        \n",
    "        print(\"âœ… Categorical encoding complete\")\n",
    "        return categorical_cols\n",
    "    \n",
    "    def create_binary_target(self):\n",
    "        \"\"\"Create binary target variable (Normal vs Attack)\"\"\"\n",
    "        \n",
    "        \n",
    "        if self.target_column in self.df.columns:\n",
    "            # Create binary classification target\n",
    "            self.df['Is_Attack'] = (self.df[self.target_column] != 'BENIGN').astype(int)\n",
    "            \n",
    "            # Also encode the multi-class target\n",
    "            self.df['Label_Encoded'] = self.label_encoder.fit_transform(self.df[self.target_column])\n",
    "            \n",
    "            attack_count = self.df['Is_Attack'].sum()\n",
    "            normal_count = len(self.df) - attack_count\n",
    "            \n",
    "            print(f\"âœ… Binary target created:\")\n",
    "            print(f\"  Normal: {normal_count:,} ({normal_count/len(self.df)*100:.1f}%)\")\n",
    "            print(f\"  Attack: {attack_count:,} ({attack_count/len(self.df)*100:.1f}%)\")\n",
    "            \n",
    "            return self.df['Is_Attack'].value_counts()\n",
    "        else:\n",
    "            print(f\"âŒ Target column '{self.target_column}' not found!\")\n",
    "            return None\n",
    "    \n",
    "    def feature_engineering(self):\n",
    "        \"\"\"Create additional features for anomaly detection\"\"\"\n",
    "        \n",
    "        \n",
    "        # List of potential feature column patterns\n",
    "        flow_features = [col for col in self.df.columns if 'flow' in col.lower()]\n",
    "        packet_features = [col for col in self.df.columns if 'packet' in col.lower()]\n",
    "        byte_features = [col for col in self.df.columns if 'byte' in col.lower() or 'length' in col.lower()]\n",
    "        \n",
    "        new_features = []\n",
    "        \n",
    "        # Example feature engineering (adjust based on actual column names)\n",
    "        try:\n",
    "            # Packet rate features\n",
    "            if any('fwd_packet' in col.lower() for col in self.df.columns):\n",
    "                fwd_cols = [col for col in self.df.columns if 'fwd_packet' in col.lower()]\n",
    "                bwd_cols = [col for col in self.df.columns if 'bwd_packet' in col.lower() or 'backward_packet' in col.lower()]\n",
    "                \n",
    "                if fwd_cols and bwd_cols:\n",
    "                    self.df['Packet_Ratio'] = (self.df[fwd_cols[0]] + 1) / (self.df[bwd_cols[0]] + 1)\n",
    "                    new_features.append('Packet_Ratio')\n",
    "            \n",
    "            # Byte rate features\n",
    "            if any('byte' in col.lower() for col in self.df.columns):\n",
    "                byte_cols = [col for col in self.df.columns if 'byte' in col.lower() and 'per' in col.lower()]\n",
    "                if byte_cols:\n",
    "                    # Create log-transformed features for better distribution\n",
    "                    for col in byte_cols[:2]:  # Limit to avoid too many features\n",
    "                        new_col = f\"Log_{col}\"\n",
    "                        self.df[new_col] = np.log1p(self.df[col].clip(lower=0))\n",
    "                        new_features.append(new_col)\n",
    "            \n",
    "            print(f\"âœ… Created {len(new_features)} new features:\")\n",
    "            for feature in new_features:\n",
    "                print(f\"  {feature}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Feature engineering partially failed: {e}\")\n",
    "        \n",
    "        return new_features\n",
    "    \n",
    "    def scale_features(self):\n",
    "        \"\"\"Scale numerical features\"\"\"\n",
    "        \n",
    "        \n",
    "        # Get feature columns (exclude target columns)\n",
    "        exclude_cols = [self.target_column, 'Is_Attack', 'Label_Encoded']\n",
    "        self.feature_columns = [col for col in self.df.columns if col not in exclude_cols]\n",
    "        \n",
    "        # Select only numeric columns for scaling\n",
    "        numeric_features = self.df[self.feature_columns].select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        if numeric_features:\n",
    "            # Fit and transform the features\n",
    "            self.df[numeric_features] = self.scaler.fit_transform(self.df[numeric_features])\n",
    "            print(f\"âœ… Scaled {len(numeric_features)} numerical features\")\n",
    "        \n",
    "        self.feature_columns = numeric_features\n",
    "        return numeric_features\n",
    "    \n",
    "    def sample_data(self, sample_size=100000, random_state=42):\n",
    "        \"\"\"Sample data for faster processing if dataset is too large\"\"\"\n",
    "        \n",
    "        if len(self.df) > sample_size:\n",
    "            # Stratified sampling to maintain class distribution\n",
    "            if 'Is_Attack' in self.df.columns:\n",
    "                normal_sample = self.df[self.df['Is_Attack'] == 0].sample(\n",
    "                    n=int(sample_size * 0.8), random_state=random_state, replace=False\n",
    "                )\n",
    "                attack_sample = self.df[self.df['Is_Attack'] == 1].sample(\n",
    "                    n=int(sample_size * 0.2), random_state=random_state, replace=False\n",
    "                )\n",
    "                self.df = pd.concat([normal_sample, attack_sample]).sample(frac=1, random_state=random_state)\n",
    "            else:\n",
    "                self.df = self.df.sample(n=sample_size, random_state=random_state)\n",
    "            \n",
    "            print(f\"âœ… Sampled to {len(self.df):,} records\")\n",
    "        else:\n",
    "            print(f\"âœ… Dataset size OK: {len(self.df):,} records\")\n",
    "        \n",
    "        return len(self.df)\n",
    "    \n",
    "    def preprocess_pipeline(self, file_path, sample_size=100000):\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "\n",
    "        \n",
    "        # Step 1: Load data\n",
    "        if not self.load_data(file_path):\n",
    "            return None\n",
    "        \n",
    "        # Step 2: Clean column names\n",
    "        self.clean_column_names()\n",
    "        \n",
    "        # Step 3: Handle missing and infinite values\n",
    "        self.handle_missing_values()\n",
    "        self.handle_infinite_values()\n",
    "        \n",
    "        # Step 4: Remove duplicates\n",
    "        self.remove_duplicates()\n",
    "        \n",
    "        # Step 5: Sample data if too large\n",
    "        self.sample_data(sample_size)\n",
    "        \n",
    "        # Step 6: Create target variables\n",
    "        self.create_binary_target()\n",
    "        \n",
    "        # Step 7: Encode categorical features\n",
    "        self.encode_categorical_features()\n",
    "        \n",
    "        # Step 8: Feature engineering\n",
    "        self.feature_engineering()\n",
    "        \n",
    "        # Step 9: Scale features\n",
    "        self.scale_features()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "        print(f\"Final dataset shape: {self.df.shape}\")\n",
    "        print(f\"Features for modeling: {len(self.feature_columns)}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def save_processed_data(self, output_path=\"processed_data.csv\"):\n",
    "        \"\"\"Save the processed dataset\"\"\"\n",
    "        if self.df is not None:\n",
    "            self.df.to_csv(output_path, index=False)\n",
    "            print(f\"ðŸ’¾ Processed data saved to: {output_path}\")\n",
    "        \n",
    "    def get_preprocessing_summary(self):\n",
    "        \"\"\"Get summary of preprocessing steps\"\"\"\n",
    "        if self.df is not None:\n",
    "            summary = {\n",
    "                'total_records': len(self.df),\n",
    "                'total_features': len(self.feature_columns) if self.feature_columns else 0,\n",
    "                'attack_percentage': self.df['Is_Attack'].mean() * 100 if 'Is_Attack' in self.df.columns else 0,\n",
    "                'feature_columns': self.feature_columns\n",
    "            }\n",
    "            return summary\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run preprocessing\"\"\"\n",
    "    print(\" CIC-IDS2017 Data Preprocessing\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = NetworkDataPreprocessor()\n",
    "    \n",
    "    file_path = r\"C:\\Users\\S\\Desktop\\SecureNet\\data\\raw\\CICIDS2017_FULL.csv\"\n",
    "    \n",
    "    # Run preprocessing pipeline\n",
    "    processed_df = preprocessor.preprocess_pipeline(file_path, sample_size=100000)\n",
    "    \n",
    "    if processed_df is not None:\n",
    "        # Save processed data\n",
    "        preprocessor.save_processed_data(\"processed_network_data.csv\")\n",
    "        \n",
    "        # Get summary\n",
    "        summary = preprocessor.get_preprocessing_summary()\n",
    "        print(\"\\nðŸ“Š PREPROCESSING SUMMARY:\")\n",
    "        print(f\"Records: {summary['total_records']:,}\")\n",
    "        print(f\"Features: {summary['total_features']}\")\n",
    "        print(f\"Attack Rate: {summary['attack_percentage']:.2f}%\")\n",
    "        \n",
    "        \n",
    "   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
